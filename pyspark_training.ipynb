{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3s1OBRabVrs",
        "outputId": "5592aeb7-1637-4ec1-9268-aa766319eb37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "irfanqs\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW1Z9sSLbap-",
        "outputId": "f8b35d2d-2ff5-41f6-c778-c7623e011d7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/irfanqs\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyspark pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDg6uORlbc0y",
        "outputId": "364fc894-e3ee-473e-af56-3d0aade3147a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyspark in ./.local/lib/python3.10/site-packages (3.5.5)\n",
            "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in ./.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gallant4114/project-kafka.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jimOi7lhIdb3",
        "outputId": "9af31de3-eeaf-43b7-e530-3f6c11be83ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'project-kafka' already exists and is not an empty directory.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "7eytEFoha4X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd project-kafka"
      ],
      "metadata": {
        "id": "CdqOJRSoJQAn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FraudCreditCard\").getOrCreate()\n",
        "\n",
        "BATCH_DIR = \"/home/irfanqs/project-kafka/batches\"\n",
        "MODEL_DIR = \"/home/irfanqs/project-kafka/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "nenHDa3MbgkB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "print(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnEMT7faY1GW",
        "outputId": "8b63068c-c3f6-4221-f9ef-90d9eb58b237"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=FraudCreditCard>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_COLUMNS = [\"amt\", \"lat\", \"long\", \"city_pop\", \"unix_time\", \"merch_lat\", \"merch_long\"]\n",
        "\n",
        "for i in range(3):\n",
        "    batch_path = os.path.join(BATCH_DIR, f\"batch_{i}.json\")\n",
        "    print(f\"Reading {batch_path}\")\n",
        "\n",
        "    df = spark.read.option(\"multiline\", \"true\").json(batch_path)\n",
        "\n",
        "    for col in FEATURE_COLUMNS:\n",
        "        df = df.withColumn(col, df[col].cast(\"double\"))\n",
        "    df = df.withColumn(\"label\", df[\"is_fraud\"].cast(\"int\"))\n",
        "\n",
        "    (trainingData, testData) = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features\")\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[assembler, lr])\n",
        "    model = pipeline.fit(trainingData) # Latih model dengan data latih\n",
        "\n",
        "    model_path = os.path.join(MODEL_DIR, f\"fraud_model_batch_{i}\")\n",
        "    model.write().overwrite().save(model_path)\n",
        "    print(f\"Model from batch {i} saved to {model_path}\")\n",
        "\n",
        "    # Evaluasi Model\n",
        "    print(f\"\\n--- Evaluating Model from batch {i} ---\")\n",
        "\n",
        "    predictions = model.transform(testData)\n",
        "    correct_predictions = predictions.filter(predictions.label == predictions.prediction).count()\n",
        "    total_predictions = predictions.count()\n",
        "\n",
        "    if total_predictions > 0:\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        print(f\"Accuracy on test data for batch {i}: {accuracy:.4f}\")\n",
        "    else:\n",
        "        print(f\"No predictions to evaluate for batch {i}.\")\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "    print(f\"Area Under ROC (AUC) on test data for batch {i}: {auc:.4f}\")\n",
        "\n",
        "print(\"\\nAll models processed.\")\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJbXtqJ8aJI1",
        "outputId": "19e7c14f-dd99-4bdf-ebd7-497a8e93f36d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /home/irfanqs/project-kafka/batches/batch_0.json\n",
            "Model from batch 0 saved to /home/irfanqs/project-kafka/models/fraud_model_batch_0\n",
            "\n",
            "--- Evaluating Model from batch 0 ---\n",
            "Accuracy on test data for batch 0: 0.9981\n",
            "Area Under ROC (AUC) on test data for batch 0: 0.9981\n",
            "Reading /home/irfanqs/project-kafka/batches/batch_1.json\n",
            "Model from batch 1 saved to /home/irfanqs/project-kafka/models/fraud_model_batch_1\n",
            "\n",
            "--- Evaluating Model from batch 1 ---\n",
            "Accuracy on test data for batch 1: 0.9905\n",
            "Area Under ROC (AUC) on test data for batch 1: 0.9302\n",
            "Reading /home/irfanqs/project-kafka/batches/batch_2.json\n",
            "Model from batch 2 saved to /home/irfanqs/project-kafka/models/fraud_model_batch_2\n",
            "\n",
            "--- Evaluating Model from batch 2 ---\n",
            "Accuracy on test data for batch 2: 0.9924\n",
            "Area Under ROC (AUC) on test data for batch 2: 0.8774\n",
            "\n",
            "All models processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari hasil yang didapat, bisa disimpulkan bahwa model dari batch 0 memiliki akurasi yang paling baik dari model dari batch 1 dan 2"
      ],
      "metadata": {
        "id": "n8duaFFnbPjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi"
      ],
      "metadata": {
        "id": "3kfXxn8hbBCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.linalg import Vector\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FraudPredictionLive\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "MODEL_DIR = \"/home/irfanqs/project-kafka/models\"\n",
        "MODEL_TO_USE = \"/home/irfanqs/project-kafka/models/fraud_model_batch_0\"\n",
        "model_path = os.path.join(MODEL_DIR, MODEL_TO_USE)\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from: {model_path}\")\n",
        "try:\n",
        "    loaded_model = PipelineModel.load(model_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    spark.stop()\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JGDrlkYpN7Fg",
        "outputId": "d09c714b-e37d-462c-bc31-271b51bc2b9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /home/irfanqs/project-kafka/models/fraud_model_batch_0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_raw = [\n",
        "    (100.50, 40.71, -74.00, 8000000, 1678886400, 40.72, -74.01),\n",
        "    (1500.00, 34.05, -118.25, 4000000, 1678887000, 34.06, -118.24),\n",
        "    (50.00, 41.88, -87.63, 2700000, 1678887600, 41.87, -87.62)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"amt\", DoubleType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"long\", DoubleType(), True),\n",
        "    StructField(\"city_pop\", IntegerType(), True),\n",
        "    StructField(\"unix_time\", IntegerType(), True),\n",
        "    StructField(\"merch_lat\", DoubleType(), True),\n",
        "    StructField(\"merch_long\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "new_df = spark.createDataFrame(new_data_raw, schema)\n",
        "\n",
        "print(\"\\nNew data for prediction:\")\n",
        "new_df.show()\n",
        "new_df.printSchema()\n",
        "\n",
        "predictions = loaded_model.transform(new_df)\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "predictions.select(\"amt\", \"lat\", \"long\", \"probability\", \"prediction\").show(truncate=False)\n",
        "print(\"1 = fraud, 0 = not fraud\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7PiNOcwb7PR",
        "outputId": "793326b3-d85f-442c-b7bc-088387da745b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New data for prediction:\n",
            "+------+-----+-------+--------+----------+---------+----------+\n",
            "|   amt|  lat|   long|city_pop| unix_time|merch_lat|merch_long|\n",
            "+------+-----+-------+--------+----------+---------+----------+\n",
            "| 100.5|40.71|  -74.0| 8000000|1678886400|    40.72|    -74.01|\n",
            "|1500.0|34.05|-118.25| 4000000|1678887000|    34.06|   -118.24|\n",
            "|  50.0|41.88| -87.63| 2700000|1678887600|    41.87|    -87.62|\n",
            "+------+-----+-------+--------+----------+---------+----------+\n",
            "\n",
            "root\n",
            " |-- amt: double (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- long: double (nullable = true)\n",
            " |-- city_pop: integer (nullable = true)\n",
            " |-- unix_time: integer (nullable = true)\n",
            " |-- merch_lat: double (nullable = true)\n",
            " |-- merch_long: double (nullable = true)\n",
            "\n",
            "\n",
            "Predictions:\n",
            "+------+-----+-------+-----------+----------+\n",
            "|amt   |lat  |long   |probability|prediction|\n",
            "+------+-----+-------+-----------+----------+\n",
            "|100.5 |40.71|-74.0  |[0.0,1.0]  |1.0       |\n",
            "|1500.0|34.05|-118.25|[0.0,1.0]  |1.0       |\n",
            "|50.0  |41.88|-87.63 |[0.0,1.0]  |1.0       |\n",
            "+------+-----+-------+-----------+----------+\n",
            "\n",
            "1 = fraud, 0 = not fraud\n"
          ]
        }
      ]
    }
  ]
}